{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "from mxnet import autograd, gluon, init\n",
    "from mxnet import ndarray as nd\n",
    "from mxnet.gluon.data import DataLoader, dataset\n",
    "from mxnet.gluon import nn, loss as gloss, data as gdata\n",
    "\n",
    "from sklearn import neighbors\n",
    "from sklearn import preprocessing\n",
    "from scipy.spatial.distance import pdist, cdist\n",
    "from numpy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = pickle.load(open('output/train/x_0.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2b73e6bd1fd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts, deletes = [], []\n",
    "for i in range(len(x_0[0])):\n",
    "    counts.append(x_0[0][i].sum())\n",
    "\n",
    "counts = pd.Series(counts)\n",
    "counts.value_counts().sort_index().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline-NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(x_0[0]).astype(np.float32)\n",
    "y = np.array(x_0[1]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pickle.load(open('output/test_500_2.pkl', 'rb'))\n",
    "x_test = np.array(test[0]).astype(np.float32)\n",
    "y_test = np.array(test[1]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for i in range(len(x)):\n",
    "    X.append(x[i].flatten())\n",
    "\n",
    "X_test = []\n",
    "for i in range(len(x_test)):\n",
    "    X_test.append(x_test[i].flatten())\n",
    "\n",
    "X = np.array(X).astype(np.float32)\n",
    "X_test = np.array(X_test).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6566, 24000) (6566,) (500, 24000) (500,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,y.shape,X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 13.352514999999812 Seconds\n",
      "0.432 0.488 0.504 0.528 0.46639909505212856\n",
      "Predicting time: 189.21340299999997 Seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "# Find Nearest Neighbors for test in train base\n",
    "mrr, acc1, acc3, acc5, acc10, length = 0.0, 0.0, 0.0, 0.0, 0.0, len(x_test)\n",
    "\n",
    "nbrs = neighbors.NearestNeighbors(n_neighbors=10, algorithm='ball_tree').fit(X)\n",
    "nbrs_all = neighbors.NearestNeighbors(n_neighbors=len(x), algorithm='ball_tree').fit(X)\n",
    "\n",
    "end_time = time.clock()\n",
    "print('Training time: %s Seconds'%(end_time-start_time))\n",
    "\n",
    "start_time = time.clock()\n",
    "distances, indices = nbrs.kneighbors(X_test)\n",
    "distances_all, indices_all = nbrs_all.kneighbors(X_test)\n",
    "    \n",
    "for i in range(len(indices)):\n",
    "        \n",
    "    y_true = y_test[i] # True label for test        \n",
    "    y_predict = y[indices[i]] # Predict for test\n",
    "    y_predict_all = y[indices_all[i]]\n",
    "        \n",
    "    rank = np.argwhere(y_predict_all == y_true)[0][0] + 1\n",
    "    mrr += 1/rank\n",
    "        \n",
    "    #print(y_true, y_predict, indices[i])\n",
    "        \n",
    "    if y_true in y_predict[:1]:\n",
    "        acc1 += 1\n",
    "        \n",
    "    if y_true in y_predict[:3]:\n",
    "        acc3 += 1\n",
    "        \n",
    "    if y_true in y_predict[:5]:\n",
    "        acc5 += 1\n",
    "        \n",
    "    if y_true in y_predict:\n",
    "        acc10 += 1\n",
    "        \n",
    "    #print(i,acc1,acc3,acc5,acc10)\n",
    "        \n",
    "acc1 /= length\n",
    "acc3 /= length\n",
    "acc5 /= length\n",
    "acc10 /= length\n",
    "mrr /= length\n",
    "\n",
    "end_time = time.clock()\n",
    "\n",
    "print(acc1,acc3,acc5,acc10,mrr)\n",
    "print('Predicting time: %s Seconds'%(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 11.842715999999882 Seconds\n",
      "0.43 0.488 0.504 0.528 0.46634770479242765\n",
      "Predicting time: 245.98267099999998 Seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "# Find Nearest Neighbors for test in train base\n",
    "mrr, acc1, acc3, acc5, acc10, length = 0.0, 0.0, 0.0, 0.0, 0.0, len(x_test)\n",
    "\n",
    "nbrs = neighbors.NearestNeighbors(n_neighbors=10, algorithm='kd_tree').fit(X)\n",
    "nbrs_all = neighbors.NearestNeighbors(n_neighbors=len(x), algorithm='kd_tree').fit(X)\n",
    "\n",
    "end_time = time.clock()\n",
    "print('Training time: %s Seconds'%(end_time-start_time))\n",
    "\n",
    "start_time = time.clock()\n",
    "distances, indices = nbrs.kneighbors(X_test)\n",
    "distances_all, indices_all = nbrs_all.kneighbors(X_test)\n",
    "    \n",
    "for i in range(len(indices)):\n",
    "        \n",
    "    y_true = y_test[i] # True label for test        \n",
    "    y_predict = y[indices[i]] # Predict for test\n",
    "    y_predict_all = y[indices_all[i]]\n",
    "        \n",
    "    rank = np.argwhere(y_predict_all == y_true)[0][0] + 1\n",
    "    mrr += 1/rank\n",
    "        \n",
    "    #print(y_true, y_predict, indices[i])\n",
    "        \n",
    "    if y_true in y_predict[:1]:\n",
    "        acc1 += 1\n",
    "        \n",
    "    if y_true in y_predict[:3]:\n",
    "        acc3 += 1\n",
    "        \n",
    "    if y_true in y_predict[:5]:\n",
    "        acc5 += 1\n",
    "        \n",
    "    if y_true in y_predict:\n",
    "        acc10 += 1\n",
    "        \n",
    "    #print(i,acc1,acc3,acc5,acc10)\n",
    "        \n",
    "acc1 /= length\n",
    "acc3 /= length\n",
    "acc5 /= length\n",
    "acc10 /= length\n",
    "mrr /= length\n",
    "\n",
    "end_time = time.clock()\n",
    "\n",
    "print(acc1,acc3,acc5,acc10,mrr)\n",
    "print('Predicting time: %s Seconds'%(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.20479199999999764 Seconds\n",
      "0.43 0.488 0.504 0.526 0.4654186546118588\n",
      "Predicting time: 9.501111999999921 Seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "# Find Nearest Neighbors for test in train base\n",
    "mrr, acc1, acc3, acc5, acc10, length = 0.0, 0.0, 0.0, 0.0, 0.0, len(x_test)\n",
    "\n",
    "nbrs = neighbors.NearestNeighbors(n_neighbors=10, algorithm='brute').fit(X)\n",
    "nbrs_all = neighbors.NearestNeighbors(n_neighbors=len(x), algorithm='brute').fit(X)\n",
    "\n",
    "end_time = time.clock()\n",
    "print('Training time: %s Seconds'%(end_time-start_time))\n",
    "\n",
    "start_time = time.clock()\n",
    "distances, indices = nbrs.kneighbors(X_test)\n",
    "distances_all, indices_all = nbrs_all.kneighbors(X_test)\n",
    "    \n",
    "for i in range(len(indices)):\n",
    "        \n",
    "    y_true = y_test[i] # True label for test        \n",
    "    y_predict = y[indices[i]] # Predict for test\n",
    "    y_predict_all = y[indices_all[i]]\n",
    "        \n",
    "    rank = np.argwhere(y_predict_all == y_true)[0][0] + 1\n",
    "    mrr += 1/rank\n",
    "        \n",
    "    #print(y_true, y_predict, indices[i])\n",
    "        \n",
    "    if y_true in y_predict[:1]:\n",
    "        acc1 += 1\n",
    "        \n",
    "    if y_true in y_predict[:3]:\n",
    "        acc3 += 1\n",
    "        \n",
    "    if y_true in y_predict[:5]:\n",
    "        acc5 += 1\n",
    "        \n",
    "    if y_true in y_predict:\n",
    "        acc10 += 1\n",
    "        \n",
    "    #print(i,acc1,acc3,acc5,acc10)\n",
    "        \n",
    "acc1 /= length\n",
    "acc3 /= length\n",
    "acc5 /= length\n",
    "acc10 /= length\n",
    "mrr /= length\n",
    "\n",
    "end_time = time.clock()\n",
    "\n",
    "print(acc1,acc3,acc5,acc10,mrr)\n",
    "print('Predicting time: %s Seconds'%(end_time-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANGE = len(x_0[0])\n",
    "SAMPLENUM = 20\n",
    "BASE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.load('output/train/indices.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "x_1 = []\n",
    "for i in range(1,SAMPLENUM+1):\n",
    "    print(i)\n",
    "    x_1.append(pickle.load(open('output/train/x_'+str(i)+'.pkl', 'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, X2, X3, Y = [], [], [], []\n",
    "for i in range(RANGE):\n",
    "    for j in range(SAMPLENUM):\n",
    "        X1.append(x_0[0][i])\n",
    "        X2.append(x_1[j][i])\n",
    "        X3.append(x_1[np.random.randint(0, SAMPLENUM)][int(indices[i][BASE+j])])\n",
    "        Y.append(x_0[1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.array(X1).astype(np.float32)\n",
    "X2 = np.array(X2).astype(np.float32)\n",
    "X3 = np.array(X3).astype(np.float32)    \n",
    "Y = np.array(Y).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(x_0[0]).astype(np.float32)\n",
    "y = np.array(x_0[1]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [[], []]\n",
    "for i in range(500):\n",
    "    a = np.random.randint(0, SAMPLENUM)\n",
    "    b = np.random.randint(0, RANGE)\n",
    "    test[0].append(x_1[a][b])\n",
    "    test[1].append(x_0[1][b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pickle.load(open('output/test_500_2.pkl', 'rb'))\n",
    "x_test = np.array(test[0]).astype(np.float32)\n",
    "y_test = np.array(test[1]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def takeFirst(elem):\n",
    "    return elem[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(num_channels):\n",
    "    blk = nn.Sequential()\n",
    "    blk.add(nn.BatchNorm(), nn.Activation('relu'),\n",
    "            nn.Conv2D(num_channels, kernel_size=3, padding=1))\n",
    "    return blk\n",
    "\n",
    "class DenseBlock(nn.Block):\n",
    "    def __init__(self, num_convs, num_channels, **kwargs):\n",
    "        super(DenseBlock, self).__init__(**kwargs)\n",
    "        self.net = nn.Sequential()\n",
    "        for _ in range(num_convs):\n",
    "            self.net.add(conv_block(num_channels))\n",
    "\n",
    "    def forward(self, X):\n",
    "        for blk in self.net:\n",
    "            Y = blk(X)\n",
    "            X = nd.concat(X, Y, dim=1)  # 在通道维上将输入和输出连结\n",
    "        return X\n",
    "\n",
    "def transition_block(num_channels):\n",
    "    blk = nn.Sequential()\n",
    "    blk.add(nn.BatchNorm(), nn.Activation('relu'),\n",
    "            nn.Conv2D(num_channels, kernel_size=1),\n",
    "            nn.AvgPool2D(pool_size=2, strides=2))\n",
    "    return blk\n",
    "\n",
    "def DenseNet():\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3),\n",
    "            nn.BatchNorm(), nn.Activation('relu'),\n",
    "            nn.MaxPool2D(pool_size=2, strides=1, padding=1))\n",
    "    \n",
    "    num_channels, growth_rate = 64, 32  # num_channels为当前的通道数\n",
    "    num_convs_in_dense_blocks = [4, 4, 4, 4]\n",
    "\n",
    "    for i, num_convs in enumerate(num_convs_in_dense_blocks):\n",
    "        net.add(DenseBlock(num_convs, growth_rate))\n",
    "        # 上一个稠密块的输出通道数\n",
    "        num_channels += num_convs * growth_rate\n",
    "        # 在稠密块之间加入通道数减半的过渡层\n",
    "        if i != len(num_convs_in_dense_blocks) - 1:\n",
    "            net.add(transition_block(num_channels // 2))\n",
    "    \n",
    "    net.add(nn.BatchNorm(), nn.Activation('relu'), nn.GlobalAvgPool2D(), nn.Dense(128))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_pyramid_pool(previous_conv, num_sample, shape, out_pool_size):\n",
    "    '''\n",
    "    previous_conv: a tensor vector of previous convolution layer\n",
    "    num_sample: an int number of image in the batch\n",
    "    previous_conv_size: an int vector [height, width] of the matrix features size of previous convolution layer\n",
    "    out_pool_size: a int vector of expected output size of max pooling layer\n",
    "    \n",
    "    returns: a tensor vector with shape [1 x n] is the concentration of multi-level pooling\n",
    "    '''\n",
    "    ## NCHW\n",
    "    N = shape[0]\n",
    "    C = shape[1]\n",
    "    H = shape[2]\n",
    "    W = shape[3]\n",
    "    \n",
    "    for i in range(len(out_pool_size)):\n",
    "        h_wid = int(math.ceil(H / out_pool_size[i]))\n",
    "        w_wid = int(math.ceil(W / out_pool_size[i]))\n",
    "        \n",
    "        h_pad = int((h_wid*out_pool_size[i] - H + 1) / 2)\n",
    "        w_pad = int((w_wid*out_pool_size[i] - W + 1) / 2)\n",
    "        \n",
    "        maxpool = nn.MaxPool2D(pool_size = (h_wid, w_wid), strides=(h_wid, w_wid), padding=(h_pad, w_pad))\n",
    "        x = maxpool(previous_conv)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        if(i == 0):\n",
    "            spp = x.reshape((N, -1))\n",
    "        else:\n",
    "            spp = nd.concat(\n",
    "                spp, \n",
    "                x.reshape((N, -1)), \n",
    "                dim=1\n",
    "            )\n",
    "    \n",
    "    return spp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alexNet():\n",
    "    net = nn.Sequential()\n",
    "    # 使用较大的11 x 11窗口来捕获物体。同时使用步幅4来较大幅度减小输出高和宽。这里使用的输出通\n",
    "    # 道数比LeNet中的也要大很多\n",
    "    net.add(nn.Conv2D(channels=96, kernel_size=11, strides=4,activation='relu'),\n",
    "            #nn.MaxPool2D(pool_size=4, strides=2),\n",
    "            # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数\n",
    "            nn.Conv2D(256, kernel_size=5, padding=2, activation='relu'),\n",
    "            #nn.MaxPool2D(pool_size=4, strides=2),\n",
    "            # 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。\n",
    "            # 前两个卷积层后不使用池化层来减小输入的高和宽\n",
    "            nn.Conv2D(384, kernel_size=5, padding=1, activation='relu'),\n",
    "            nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),\n",
    "            nn.Conv2D(256, kernel_size=3, padding=1, activation='relu'),\n",
    "            nn.MaxPool2D(pool_size=3, strides=2),\n",
    "            # 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合\n",
    "            nn.Dense(4096, activation=\"relu\"), nn.Dropout(0.5),\n",
    "            nn.Dense(4096, activation=\"relu\"), nn.Dropout(0.5),\n",
    "            nn.Dense(128))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_block(num_convs, num_channels):\n",
    "    blk = nn.Sequential()\n",
    "    for _ in range(num_convs):\n",
    "        blk.add(nn.Conv2D(num_channels, kernel_size=3, padding=1, activation='relu'))\n",
    "    blk.add(nn.MaxPool2D(pool_size=2, strides=2, padding=1))\n",
    "    return blk\n",
    "\n",
    "def vgg(conv_arch):\n",
    "    net = nn.Sequential()\n",
    "    # 卷积层部分\n",
    "    for (num_convs, num_channels) in conv_arch:\n",
    "        net.add(vgg_block(num_convs, num_channels))\n",
    "    # 全连接层部分\n",
    "    net.add(nn.Dense(4096, activation='relu'), nn.Dropout(0.5),\n",
    "            nn.Dense(4096, activation='relu'), nn.Dropout(0.5),\n",
    "            nn.Dense(128))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseNet():\n",
    "    base_net = nn.Sequential()\n",
    "    with base_net.name_scope():\n",
    "        base_net.add(nn.Dense(256, activation='relu'))\n",
    "        base_net.add(nn.Dense(128, activation='relu'))\n",
    "    return base_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArrayDataset():\n",
    "    \"\"\"A dataset that combines multiple dataset-like objects, e.g.\n",
    "    Datasets, lists, arrays, etc.\n",
    "\n",
    "    The i-th sample is defined as `(x1[i], x2[i], ...)`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    *args : one or more dataset-like objects\n",
    "        The data arrays.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        assert len(args) > 0, \"Needs at least 1 arrays\"\n",
    "        self._length = len(args[0])\n",
    "        self._data = []\n",
    "        for i, data in enumerate(args):\n",
    "            assert len(data) == self._length, \\\n",
    "                \"All arrays must have the same length; array[0] has length %d \" \\\n",
    "                \"while array[%d] has %d.\" % (self._length, i+1, len(data))\n",
    "            if isinstance(data, nd.NDArray) and len(data.shape) == 1:\n",
    "                data = data.asnumpy()\n",
    "            self._data.append(data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if len(Xself._data) == 1:\n",
    "            return self._data[0][idx]\n",
    "        else:\n",
    "            return tuple(data[idx] for data in self._data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_net(net, train_iter, test_iter, ctx):\n",
    "    \n",
    "    # Predict for train\n",
    "    for i, (data, label) in enumerate(train_iter):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        \n",
    "        if i == 0:\n",
    "            X = net(data).asnumpy()\n",
    "            Y = label.asnumpy()\n",
    "        else:\n",
    "            X = np.concatenate((X, net(data).asnumpy()))\n",
    "            Y = np.concatenate((Y, label.asnumpy()))\n",
    "\n",
    "    #normalizer = preprocessing.Normalizer().fit(X)\n",
    "    #X = normalizer.transform(X)\n",
    "    nbrs = neighbors.NearestNeighbors(n_neighbors=10, algorithm='ball_tree').fit(X)\n",
    "    nbrs_all = neighbors.NearestNeighbors(n_neighbors=len(X), algorithm='ball_tree').fit(X)\n",
    "    \n",
    "    # Predict for test\n",
    "    for i, (test, label) in enumerate(test_iter):\n",
    "        test = test.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        \n",
    "        if i == 0:\n",
    "            X_test = net(test).asnumpy()\n",
    "            Y_test = label.asnumpy()\n",
    "        else:\n",
    "            X_test = np.concatenate((X_test, net(test).asnumpy()))\n",
    "            Y_test = np.concatenate((Y_test, label.asnumpy()))\n",
    "            \n",
    "    start_time = time.clock()\n",
    "    \n",
    "    # Find Nearest Neighbors for test in train base\n",
    "    mrr, acc1, acc3, acc5, acc10, length = 0.0, 0.0, 0.0, 0.0, 0.0, len(X_test)\n",
    "    #X_test = normalizer.transform(X_test)    \n",
    "    distances, indices = nbrs.kneighbors(X_test)\n",
    "    distances_all, indices_all = nbrs_all.kneighbors(X_test)\n",
    "    \n",
    "    for i in range(len(indices)):\n",
    "        \n",
    "        y_true = Y_test[i] # True label for test        \n",
    "        y_predict = Y[indices[i]] # Predict for test\n",
    "        y_predict_all = Y[indices_all[i]]\n",
    "        \n",
    "        rank = np.argwhere(y_predict_all == y_true)[0][0] + 1\n",
    "        mrr += 1/rank\n",
    "        \n",
    "        #print(y_true, y_predict, indices[i])\n",
    "        \n",
    "        if y_true in y_predict[:1]:\n",
    "            acc1 += 1\n",
    "        \n",
    "        if y_true in y_predict[:3]:\n",
    "            acc3 += 1\n",
    "        \n",
    "        if y_true in y_predict[:5]:\n",
    "            acc5 += 1\n",
    "        \n",
    "        if y_true in y_predict:\n",
    "            acc10 += 1\n",
    "    \n",
    "    end_time = time.clock()\n",
    "        \n",
    "    acc1 /= length\n",
    "    acc3 /= length\n",
    "    acc5 /= length\n",
    "    acc10 /= length\n",
    "    mrr /= length\n",
    "    \n",
    "    predict_time = end_time - start_time\n",
    "    \n",
    "    return [mrr,acc1,acc3,acc5,acc10,predict_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "def save(params, filename, strip_prefix=''):\n",
    "    \"\"\"Save parameters to file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        Path to parameter file.\n",
    "    strip_prefix : str, default ''\n",
    "        Strip prefix from parameter names before saving.\n",
    "    \"\"\"\n",
    "    arg_dict = {}\n",
    "    for param in params.values():\n",
    "        weight = param._reduce()\n",
    "        if not param.name.startswith(strip_prefix):\n",
    "            raise ValueError(\n",
    "                \"Prefix '%s' is to be striped before saving, but Parameter's \"\n",
    "                \"name '%s' does not start with '%s'. \"\n",
    "                \"this may be due to your Block shares parameters from other \"\n",
    "                \"Blocks or you forgot to use 'with name_scope()' when creating \"\n",
    "                \"child blocks. For more info on naming, please see \"\n",
    "                \"http://mxnet.incubator.apache.org/tutorials/basic/naming.html\"%(\n",
    "                    strip_prefix, param.name, strip_prefix))\n",
    "        arg_dict[param.name[len(strip_prefix):]] = weight\n",
    "    \n",
    "    #print(arg_dict)   \n",
    "    mx.nd.save(filename, arg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPP_CNN(nn.Block):\n",
    "    '''\n",
    "    A CNN model which adds spp layer so that we can input multi-size tensor\n",
    "    '''\n",
    "    def __init__(self, d=218, **kwargs):\n",
    "        super(SPP_CNN, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.output_num = [4, 2, 1]\n",
    "            self.d = d\n",
    "\n",
    "            self.conv1 = nn.Conv2D(channels=96, kernel_size=11, strides=4,activation='relu')\n",
    "\n",
    "            self.conv2 = nn.Conv2D(channels=256, kernel_size=7, padding=2, activation='relu')\n",
    "            self.BN1 = nn.BatchNorm()\n",
    "\n",
    "            self.conv3 = nn.Conv2D(channels=384, kernel_size=5, padding=1, activation='relu')\n",
    "            self.BN2 = nn.BatchNorm()\n",
    "\n",
    "            #self.conv4 = nn.Conv2D(channels=384, kernel_size=3, padding=1, activation='relu')\n",
    "            #self.BN3 = nn.BatchNorm()\n",
    "\n",
    "            #self.conv5 = nn.Conv2D(channels=256, kernel_size=3, padding=1, activation='relu')\n",
    "            #self.BN4 = nn.BatchNorm()\n",
    "\n",
    "#             self.pool1 = nn.MaxPool2D(pool_size=3, strides=2)\n",
    "    #         self.pool2 = nn.MaxPool2D(pool_size=3, strides=2)\n",
    "    #         self.pool3 = nn.MaxPool2D(pool_size=3, strides=2)\n",
    "\n",
    "            self.drop = nn.Dropout(0.3)\n",
    "\n",
    "            self.fc1 = nn.Dense(4096)\n",
    "            self.fc2 = nn.Dense(d)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.BN1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.BN2(x)\n",
    "        #x = self.pool1(x)\n",
    "        x = spatial_pyramid_pool(x, 1, x.shape, self.output_num)        \n",
    "        fc1 = self.fc1(self.drop(x))\n",
    "        fc2 = self.fc2(fc1)\n",
    "        \n",
    "        return fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project(x, y, X1, X2, X3, Y, x_test, y_test):\n",
    "    \n",
    "    ctx = mx.gpu()\n",
    "    batch_size = 512\n",
    "    random.seed(47)\n",
    "    \n",
    "    ArrayDataset(X1, X2, X3, Y)\n",
    "    train_iter = gdata.DataLoader(gdata.ArrayDataset(X1, X2, X3, Y), batch_size, shuffle=True)    \n",
    "    test_iter = gdata.DataLoader(gdata.ArrayDataset(x_test, y_test), batch_size, shuffle=True)\n",
    "    x_iter = gdata.DataLoader(gdata.ArrayDataset(x, y), batch_size, shuffle=False)\n",
    "    print('Data Load Success!')\n",
    "    \n",
    "#     net = DenseNet()\n",
    "    \n",
    "#     conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))\n",
    "#     net = vgg(conv_arch)\n",
    "    net = alexNet()\n",
    "#     net = SPP_CNN()\n",
    "#     net.hybridize()\n",
    "#     net = baseNet() \n",
    "    #net.collect_params().initialize(mx.init.Uniform(scale=0.1), ctx=ctx) \n",
    "    \n",
    "    net.initialize(ctx=ctx, init=init.Xavier())\n",
    "    print('Build Net Success!')\n",
    "\n",
    "    triplet_loss = gloss.TripletLoss()\n",
    "    trainer_triplet = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.0001})\n",
    "    \n",
    "    print('---------------->')\n",
    "    best_mrr = 0\n",
    "    predict_time = []\n",
    "    for epoch in range(10):      \n",
    "        curr_loss = 0.0\n",
    "        for i, (a, b, c, label) in enumerate(train_iter):            \n",
    "            a = a.as_in_context(ctx)\n",
    "            b = b.as_in_context(ctx)\n",
    "            c = c.as_in_context(ctx)            \n",
    "            \n",
    "            anc_ins, pos_ins, neg_ins = a, b, c\n",
    "            with autograd.record():\n",
    "                inter1 = net(anc_ins)\n",
    "                inter2 = net(pos_ins)\n",
    "                inter3 = net(neg_ins)\n",
    "                loss = triplet_loss(inter1, inter2, inter3)  # Triplet Loss\n",
    "            \n",
    "            loss.backward()\n",
    "            trainer_triplet.step(batch_size)\n",
    "            curr_loss = mx.nd.mean(loss).asscalar()\n",
    "            # print('Epoch: %s, Batch: %s, Triplet Loss: %s' % (epoch, i, curr_loss))\n",
    "        \n",
    "        print('Epoch: %s, Triplet Loss: %s' % (epoch, curr_loss)) \n",
    "        evas = evaluate_net(net, x_iter, test_iter, ctx) #mrr,acc1,acc3,acc5,acc10\n",
    "        print(evas[0], ' |', evas[1],' |', evas[2],' |', evas[3], ' |', evas[4])\n",
    "        predict_time.append(evas[5])\n",
    "        \n",
    "        #if evas[0] > best_mrr:\n",
    "            #best_mrr = evas[0]\n",
    "            #net.collect_params().save('SPP+CNN.params')\n",
    "            #net.save_params('SPP+CNN.params')\n",
    "            #pickle.dump(net.collect_params(), open('SPP+CNN.pkl', 'wb'))\n",
    "            #save(net.collect_params(), 'SPP+CNN.params')\n",
    "            #net.export(\"net\", epoch=2)\n",
    "    \n",
    "    predict_time = np.array(predict_time).mean()\n",
    "    print('Running time: %s Seconds'%(predict_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_dimention(x, y, X1, X2, X3, Y, x_test, y_test, d):\n",
    "    \n",
    "    ctx = mx.gpu()\n",
    "    batch_size = 512\n",
    "    random.seed(47)\n",
    "    \n",
    "    ArrayDataset(X1, X2, X3, Y)\n",
    "    train_iter = gdata.DataLoader(gdata.ArrayDataset(X1, X2, X3, Y), batch_size, shuffle=True)    \n",
    "    test_iter = gdata.DataLoader(gdata.ArrayDataset(x_test, y_test), batch_size, shuffle=True)\n",
    "    x_iter = gdata.DataLoader(gdata.ArrayDataset(x, y), batch_size, shuffle=False)\n",
    "    print('Data Load Success!')\n",
    "    \n",
    "#     net = DenseNet()\n",
    "    \n",
    "#     conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))\n",
    "#     net = vgg(conv_arch)\n",
    "#     net = alexNet()\n",
    "    net = SPP_CNN(d)\n",
    "#     net.hybridize()\n",
    "#     net = baseNet() \n",
    "    #net.collect_params().initialize(mx.init.Uniform(scale=0.1), ctx=ctx) \n",
    "    \n",
    "    net.initialize(ctx=ctx, init=init.Xavier())\n",
    "    print('Build Net Success!')\n",
    "\n",
    "    triplet_loss = gloss.TripletLoss()\n",
    "    trainer_triplet = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.0001})\n",
    "    \n",
    "    print('---------------->')\n",
    "    best_mrr = 0\n",
    "    predict_time = []\n",
    "    for epoch in range(10):      \n",
    "        curr_loss = 0.0\n",
    "        for i, (a, b, c, label) in enumerate(train_iter):            \n",
    "            a = a.as_in_context(ctx)\n",
    "            b = b.as_in_context(ctx)\n",
    "            c = c.as_in_context(ctx)            \n",
    "            \n",
    "            anc_ins, pos_ins, neg_ins = a, b, c\n",
    "            with autograd.record():\n",
    "                inter1 = net(anc_ins)\n",
    "                inter2 = net(pos_ins)\n",
    "                inter3 = net(neg_ins)\n",
    "                loss = triplet_loss(inter1, inter2, inter3)  # Triplet Loss\n",
    "            \n",
    "            loss.backward()\n",
    "            trainer_triplet.step(batch_size)\n",
    "            curr_loss = mx.nd.mean(loss).asscalar()\n",
    "            # print('Epoch: %s, Batch: %s, Triplet Loss: %s' % (epoch, i, curr_loss))\n",
    "        \n",
    "        print('Epoch: %s, Triplet Loss: %s' % (epoch, curr_loss)) \n",
    "        evas = evaluate_net(net, x_iter, test_iter, ctx) #mrr,acc1,acc3,acc5,acc10\n",
    "        print(evas[0], ' |', evas[1],' |', evas[2],' |', evas[3], ' |', evas[4])\n",
    "        predict_time.append(evas[5])\n",
    "        \n",
    "        #if evas[0] > best_mrr:\n",
    "            #best_mrr = evas[0]\n",
    "            #net.collect_params().save('SPP+CNN.params')\n",
    "            #net.save_params('SPP+CNN.params')\n",
    "            #pickle.dump(net.collect_params(), open('SPP+CNN.pkl', 'wb'))\n",
    "            #save(net.collect_params(), 'SPP+CNN.params')\n",
    "            #net.export(\"net\", epoch=2)\n",
    "    \n",
    "    predict_time = np.array(predict_time).mean()\n",
    "    print('Running time: %s Seconds'%(predict_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Load Success!\n",
      "Build Net Success!\n",
      "---------------->\n",
      "Epoch: 0, Triplet Loss: 0.05748183\n",
      "0.33896669611434826  | 0.28  | 0.376  | 0.396  | 0.44\n",
      "Epoch: 1, Triplet Loss: 0.05593499\n",
      "0.3801647845234915  | 0.332  | 0.406  | 0.434  | 0.468\n",
      "Epoch: 2, Triplet Loss: 0.047013484\n",
      "0.3639525871118377  | 0.314  | 0.388  | 0.408  | 0.45\n",
      "Epoch: 3, Triplet Loss: 0.03115404\n",
      "0.3845119723465649  | 0.342  | 0.398  | 0.422  | 0.472\n",
      "Epoch: 4, Triplet Loss: 0.06300057\n",
      "0.369451400241989  | 0.326  | 0.392  | 0.416  | 0.444\n",
      "Epoch: 5, Triplet Loss: 0.029189372\n",
      "0.36257050438803495  | 0.312  | 0.388  | 0.422  | 0.45\n",
      "Epoch: 6, Triplet Loss: 0.014166373\n",
      "0.3555083419443531  | 0.31  | 0.374  | 0.4  | 0.438\n",
      "Epoch: 7, Triplet Loss: 0.013283009\n",
      "0.3655007846426245  | 0.314  | 0.396  | 0.416  | 0.466\n",
      "Epoch: 8, Triplet Loss: 0.017639471\n",
      "0.36980247625113  | 0.32  | 0.386  | 0.422  | 0.466\n",
      "Epoch: 9, Triplet Loss: 0.015168813\n",
      "0.36676623100537575  | 0.314  | 0.392  | 0.434  | 0.47\n",
      "Running time: 1.482315899999992 Seconds\n"
     ]
    }
   ],
   "source": [
    "# baseNet MLP\n",
    "project(x, y, X1, X2, X3, Y, x_test, y_test) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Load Success!\n",
      "Build Net Success!\n",
      "---------------->\n",
      "Epoch: 0, Triplet Loss: 1.0574257\n",
      "0.5404037916147338  | 0.48  | 0.566  | 0.62  | 0.672\n",
      "Epoch: 1, Triplet Loss: 12.189172\n",
      "0.5551979712693433  | 0.482  | 0.596  | 0.65  | 0.696\n",
      "Epoch: 2, Triplet Loss: 2.1697836\n",
      "0.5968348598670644  | 0.534  | 0.622  | 0.666  | 0.716\n",
      "Epoch: 3, Triplet Loss: 8.581271\n",
      "0.5998539421544212  | 0.532  | 0.64  | 0.666  | 0.714\n",
      "Epoch: 4, Triplet Loss: 0.62379014\n",
      "0.6013133456055388  | 0.536  | 0.632  | 0.68  | 0.726\n",
      "Epoch: 5, Triplet Loss: 0.4765452\n",
      "0.6414196641974217  | 0.576  | 0.678  | 0.714  | 0.752\n",
      "Epoch: 6, Triplet Loss: 1.9760455\n",
      "0.609560500178526  | 0.546  | 0.644  | 0.692  | 0.734\n",
      "Epoch: 7, Triplet Loss: 0.0\n",
      "0.6358650546631764  | 0.582  | 0.658  | 0.698  | 0.75\n",
      "Epoch: 8, Triplet Loss: 0.0\n",
      "0.6297510589032125  | 0.566  | 0.664  | 0.694  | 0.736\n",
      "Epoch: 9, Triplet Loss: 0.115688525\n",
      "0.6199977645797694  | 0.568  | 0.638  | 0.668  | 0.732\n",
      "Running time: 1.483275699999922 Seconds\n"
     ]
    }
   ],
   "source": [
    "# CNN+SPP\n",
    "project(x, y, X1, X2, X3, Y, x_test, y_test) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Load Success!\n",
      "Build Net Success!\n",
      "---------------->\n",
      "Epoch: 0, Triplet Loss: 0.3544389\n",
      "0.0171392692497552  | 0.008  | 0.01  | 0.02  | 0.03\n",
      "Epoch: 1, Triplet Loss: 0.34064183\n",
      "0.022094645803914018  | 0.01  | 0.024  | 0.028  | 0.03\n",
      "Epoch: 2, Triplet Loss: 0.32877266\n",
      "0.0283796782464776  | 0.014  | 0.028  | 0.032  | 0.044\n",
      "Epoch: 3, Triplet Loss: 0.13939273\n",
      "0.03585782170755398  | 0.014  | 0.028  | 0.034  | 0.06\n",
      "Epoch: 4, Triplet Loss: 0.15885502\n",
      "0.02946155465788756  | 0.004  | 0.018  | 0.044  | 0.07\n",
      "Epoch: 5, Triplet Loss: 0.21264145\n",
      "0.041243682104590056  | 0.016  | 0.032  | 0.046  | 0.078\n",
      "Epoch: 6, Triplet Loss: 0.16375843\n",
      "0.03453407157513352  | 0.008  | 0.02  | 0.03  | 0.072\n",
      "Epoch: 7, Triplet Loss: 0.08338602\n",
      "0.04237494000731938  | 0.012  | 0.036  | 0.054  | 0.078\n",
      "Epoch: 8, Triplet Loss: 0.12809478\n",
      "0.03714770138138281  | 0.012  | 0.024  | 0.034  | 0.064\n",
      "Epoch: 9, Triplet Loss: 0.051326178\n",
      "0.03479277454743044  | 0.008  | 0.022  | 0.042  | 0.068\n"
     ]
    }
   ],
   "source": [
    "# vgg\n",
    "project(x, y, X1, X2, X3, Y, x_test, y_test) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Load Success!\n",
      "Build Net Success!\n",
      "---------------->\n",
      "Epoch: 0, Triplet Loss: 0.8404731\n",
      "0.3652207034062965  | 0.292  | 0.39  | 0.428  | 0.494\n",
      "Epoch: 1, Triplet Loss: 0.09429787\n",
      "0.39998121125007974  | 0.326  | 0.432  | 0.48  | 0.538\n",
      "Epoch: 2, Triplet Loss: 0.2510015\n",
      "0.44879599524158603  | 0.366  | 0.484  | 0.528  | 0.6\n",
      "Epoch: 3, Triplet Loss: 0.13852544\n",
      "0.464679765851558  | 0.39  | 0.484  | 0.554  | 0.608\n",
      "Epoch: 4, Triplet Loss: 0.05552167\n",
      "0.4816901232783938  | 0.404  | 0.512  | 0.564  | 0.636\n",
      "Epoch: 5, Triplet Loss: 0.07329809\n",
      "0.48877342203862184  | 0.408  | 0.522  | 0.59  | 0.662\n",
      "Epoch: 6, Triplet Loss: 0.004454679\n",
      "0.499692545946783  | 0.414  | 0.542  | 0.586  | 0.644\n",
      "Epoch: 7, Triplet Loss: 0.013603657\n",
      "0.49834918114022153  | 0.422  | 0.536  | 0.586  | 0.654\n",
      "Epoch: 8, Triplet Loss: 0.102632135\n",
      "0.49272386649286937  | 0.41  | 0.526  | 0.582  | 0.658\n",
      "Epoch: 9, Triplet Loss: 0.021918485\n",
      "0.5069912411090075  | 0.43  | 0.532  | 0.588  | 0.65\n",
      "Running time: 1.4822194999998828 Seconds\n"
     ]
    }
   ],
   "source": [
    "# cnn without spp\n",
    "project(x, y, X1, X2, X3, Y, x_test, y_test) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Load Success!\n",
      "Build Net Success!\n",
      "---------------->\n",
      "Epoch: 0, Triplet Loss: 0.39489725\n",
      "0.3593629833199123  | 0.29  | 0.386  | 0.426  | 0.474\n",
      "Epoch: 1, Triplet Loss: 0.27311426\n",
      "0.40839078457002614  | 0.346  | 0.432  | 0.476  | 0.522\n",
      "Epoch: 2, Triplet Loss: 0.19635671\n",
      "0.45557784138156376  | 0.386  | 0.486  | 0.52  | 0.588\n",
      "Epoch: 3, Triplet Loss: 0.14930607\n",
      "0.45307056795664036  | 0.368  | 0.502  | 0.54  | 0.592\n",
      "Epoch: 4, Triplet Loss: 0.08219996\n",
      "0.48832361886895415  | 0.41  | 0.518  | 0.564  | 0.64\n",
      "Epoch: 5, Triplet Loss: 0.031281315\n",
      "0.49296712932806436  | 0.414  | 0.526  | 0.584  | 0.642\n",
      "Epoch: 6, Triplet Loss: 0.0023213422\n",
      "0.5147549570745591  | 0.444  | 0.54  | 0.584  | 0.662\n",
      "Epoch: 7, Triplet Loss: 0.005821974\n",
      "0.5127627968631281  | 0.432  | 0.558  | 0.6  | 0.676\n",
      "Epoch: 8, Triplet Loss: 0.033547647\n",
      "0.5103186555659616  | 0.43  | 0.548  | 0.598  | 0.65\n",
      "Epoch: 9, Triplet Loss: 0.04915521\n",
      "0.4913074882684889  | 0.42  | 0.516  | 0.566  | 0.63\n"
     ]
    }
   ],
   "source": [
    "# spp_cnn 64\n",
    "project(x, y, X1, X2, X3, Y, x_test, y_test) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Load Success!\n",
      "Build Net Success!\n",
      "---------------->\n",
      "Epoch: 0, Triplet Loss: 6.312211\n",
      "0.3625749910045747  | 0.286  | 0.396  | 0.428  | 0.498\n",
      "Epoch: 1, Triplet Loss: 10.240993\n",
      "0.4414656514087155  | 0.368  | 0.474  | 0.522  | 0.584\n",
      "Epoch: 2, Triplet Loss: 1.5730058\n",
      "0.4684083027438756  | 0.384  | 0.496  | 0.572  | 0.646\n",
      "Epoch: 3, Triplet Loss: 4.003873\n",
      "0.4663273400067392  | 0.388  | 0.496  | 0.55  | 0.62\n",
      "Epoch: 4, Triplet Loss: 1.9408531\n",
      "0.49299108605026143  | 0.412  | 0.53  | 0.586  | 0.648\n",
      "Epoch: 5, Triplet Loss: 1.2305316\n",
      "0.4988914982741626  | 0.416  | 0.538  | 0.588  | 0.654\n",
      "Epoch: 6, Triplet Loss: 0.26069236\n",
      "0.46948380826406383  | 0.396  | 0.506  | 0.558  | 0.6\n",
      "Epoch: 7, Triplet Loss: 0.0\n",
      "0.5007973660619676  | 0.436  | 0.514  | 0.572  | 0.624\n",
      "Epoch: 8, Triplet Loss: 0.98329544\n",
      "0.4669172380617659  | 0.392  | 0.498  | 0.55  | 0.61\n",
      "Epoch: 9, Triplet Loss: 0.0\n",
      "0.4496086613306121  | 0.374  | 0.482  | 0.538  | 0.588\n"
     ]
    }
   ],
   "source": [
    "# spp_cnn 32\n",
    "project(x, y, X1, X2, X3, Y, x_test, y_test) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Load Success!\n",
      "Build Net Success!\n",
      "---------------->\n",
      "Epoch: 0, Triplet Loss: 0.99964106\n",
      "0.13953854088447526  | 0.108  | 0.152  | 0.168  | 0.196\n",
      "Epoch: 1, Triplet Loss: 0.99510497\n",
      "0.1368170878312362  | 0.104  | 0.15  | 0.168  | 0.196\n",
      "Epoch: 2, Triplet Loss: 0.99654603\n",
      "0.1367225897497655  | 0.106  | 0.148  | 0.166  | 0.192\n",
      "Epoch: 3, Triplet Loss: 0.9984805\n",
      "0.1347471251857719  | 0.104  | 0.146  | 0.166  | 0.186\n",
      "Epoch: 4, Triplet Loss: 0.99585825\n",
      "0.13317837605710398  | 0.102  | 0.144  | 0.162  | 0.186\n",
      "Epoch: 5, Triplet Loss: 0.9954663\n",
      "0.13217355762466956  | 0.1  | 0.144  | 0.162  | 0.186\n",
      "Epoch: 6, Triplet Loss: 1.0011123\n",
      "0.1305849720695378  | 0.1  | 0.144  | 0.158  | 0.186\n",
      "Epoch: 7, Triplet Loss: 0.99677783\n",
      "0.12863030517929422  | 0.098  | 0.142  | 0.156  | 0.182\n",
      "Epoch: 8, Triplet Loss: 0.99871874\n",
      "0.12742372228190782  | 0.098  | 0.14  | 0.156  | 0.176\n",
      "Epoch: 9, Triplet Loss: 0.997101\n",
      "0.12670435757420587  | 0.098  | 0.136  | 0.156  | 0.174\n",
      "Running time: 1.414435500000036 Seconds\n"
     ]
    }
   ],
   "source": [
    "# alexnet\n",
    "project(x, y, X1, X2, X3, Y, x_test, y_test) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Load Success!\n",
      "Build Net Success!\n",
      "---------------->\n",
      "Epoch: 0, Triplet Loss: 0.07956708\n",
      "0.26748365481350755  | 0.196  | 0.298  | 0.336  | 0.404\n",
      "Epoch: 1, Triplet Loss: 0.007817073\n",
      "0.29395849446892625  | 0.22  | 0.33  | 0.364  | 0.42\n",
      "Epoch: 2, Triplet Loss: 0.04763902\n",
      "0.32711167469268077  | 0.248  | 0.366  | 0.408  | 0.464\n",
      "Epoch: 3, Triplet Loss: 0.13865106\n",
      "0.34204625581112896  | 0.27  | 0.37  | 0.42  | 0.48\n",
      "Epoch: 4, Triplet Loss: 0.017503103\n",
      "0.33373705984884194  | 0.258  | 0.368  | 0.41  | 0.466\n",
      "Epoch: 5, Triplet Loss: 0.0090175355\n",
      "0.36694371091444383  | 0.302  | 0.39  | 0.434  | 0.486\n",
      "Epoch: 6, Triplet Loss: 0.0071848677\n",
      "0.3532540148937235  | 0.274  | 0.39  | 0.432  | 0.494\n",
      "Epoch: 7, Triplet Loss: 0.0\n",
      "0.37417948552659164  | 0.294  | 0.41  | 0.446  | 0.506\n",
      "Epoch: 8, Triplet Loss: 0.01272564\n",
      "0.3718448255922297  | 0.31  | 0.4  | 0.43  | 0.47\n",
      "Epoch: 9, Triplet Loss: 0.027893284\n",
      "0.36826230871672044  | 0.288  | 0.406  | 0.452  | 0.5\n",
      "Running time: 1.516698299999979 Seconds\n"
     ]
    }
   ],
   "source": [
    "#DenseNet\n",
    "project(x, y, X1, X2, X3, Y, x_test, y_test) #"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
